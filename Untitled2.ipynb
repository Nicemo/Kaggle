{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "data_path = '/home/huangcr/Kaggle/wsdm/data/'\n",
    "train = pd.read_csv(data_path+'train.csv',dtype={'msno' : 'category','source_system_tab' : 'category',\\\n",
    "                                            'source_screen_name' : 'category','source_type' : 'category',\\\n",
    "                                            'target' : np.uint8,'song_id' : 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(data_path + 'test.csv', dtype={'msno' : 'category','source_system_tab' : 'category',\n",
    "                                                'source_screen_name' : 'category',\n",
    "                                                'source_type' : 'category',\n",
    "                                                'song_id' : 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songs = pd.read_csv(data_path + 'songs.csv',dtype={'genre_ids': 'category',\n",
    "                                                  'language' : 'category',\n",
    "                                                  'artist_name' : 'category',\n",
    "                                                  'composer' : 'category',\n",
    "                                                  'lyricist' : 'category',\n",
    "                                                  'song_id' : 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "members = pd.read_csv(data_path + 'members.csv',dtype={'city' : 'category',\n",
    "                                                      'bd' : np.uint8,\n",
    "                                                      'gender' : 'category',\n",
    "                                                      'registered_via' : 'category'})\n",
    "songs_extra = pd.read_csv(data_path + 'song_extra_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing...\n"
     ]
    }
   ],
   "source": [
    "print('Data preprocessing...')\n",
    "song_cols = ['song_id', 'artist_name', 'genre_ids', 'song_length', 'language']\n",
    "train = train.merge(songs[song_cols], on='song_id', how='left')\n",
    "test = test.merge(songs[song_cols], on='song_id', how='left')\n",
    "\n",
    "members['registration_year'] = members['registration_init_time'].apply(lambda x: int(str(x)[0:4]))\n",
    "members['registration_month'] = members['registration_init_time'].apply(lambda x: int(str(x)[4:6]))\n",
    "members['registration_date'] = members['registration_init_time'].apply(lambda x: int(str(x)[6:8]))\n",
    "\n",
    "members['expiration_year'] = members['expiration_date'].apply(lambda x: int(str(x)[0:4]))\n",
    "members['expiration_month'] = members['expiration_date'].apply(lambda x: int(str(x)[4:6]))\n",
    "members['expiration_date'] = members['expiration_date'].apply(lambda x: int(str(x)[6:8]))\n",
    "members = members.drop(['registration_init_time'], axis=1)\n",
    "\n",
    "def isrc_to_year(isrc):\n",
    "    if type(isrc) == str:\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan\n",
    "        \n",
    "songs_extra['song_year'] = songs_extra['isrc'].apply(isrc_to_year)\n",
    "songs_extra.drop(['isrc', 'name'], axis = 1, inplace = True)\n",
    "\n",
    "train = train.merge(members, on='msno', how='left')\n",
    "test = test.merge(members, on='msno', how='left')\n",
    "\n",
    "train = train.merge(songs_extra, on = 'song_id', how = 'left')\n",
    "test = test.merge(songs_extra, on = 'song_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del members, songs; gc.collect();\n",
    "\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == object:\n",
    "        train[col] = train[col].astype('category')\n",
    "        test[col] = test[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X = train.drop(['target'], axis=1)\n",
    "y = train['target'].values\n",
    "\n",
    "loc = int(train.shape[0] * 0.7)\n",
    "\n",
    "X = train.drop(['target'], axis=1)\n",
    "y = train['target'].values\n",
    "\n",
    "X0 = X[:loc]\n",
    "y0 = y[:loc]\n",
    "\n",
    "X1 = X[loc:]\n",
    "y1 = y[loc:]\n",
    "\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "ids = test['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train.drop(['target'], axis=1)\n",
    "y = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = X[-2556790:-1]\n",
    "y1 = y[-2556790:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "del train, test; gc.collect();\n",
    "\n",
    "d_train = lgb.Dataset(X, y)\n",
    "dev =lgb.Dataset(X1, y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train, test; gc.collect();\n",
    "\n",
    "df_train = lgb.Dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n",
      "[5]\ttraining's auc: 0.721661\n",
      "[10]\ttraining's auc: 0.73637\n",
      "[15]\ttraining's auc: 0.746324\n",
      "[20]\ttraining's auc: 0.752153\n",
      "[25]\ttraining's auc: 0.757147\n",
      "[30]\ttraining's auc: 0.761097\n",
      "[35]\ttraining's auc: 0.764774\n",
      "[40]\ttraining's auc: 0.767876\n",
      "[45]\ttraining's auc: 0.770066\n",
      "[50]\ttraining's auc: 0.77212\n",
      "[55]\ttraining's auc: 0.773455\n",
      "[60]\ttraining's auc: 0.774315\n",
      "[65]\ttraining's auc: 0.775214\n",
      "[70]\ttraining's auc: 0.776497\n",
      "[75]\ttraining's auc: 0.777802\n",
      "[80]\ttraining's auc: 0.778805\n",
      "[85]\ttraining's auc: 0.779972\n",
      "[90]\ttraining's auc: 0.781384\n",
      "[95]\ttraining's auc: 0.782812\n",
      "[100]\ttraining's auc: 0.783916\n",
      "[105]\ttraining's auc: 0.785125\n",
      "[110]\ttraining's auc: 0.785941\n",
      "[115]\ttraining's auc: 0.786796\n",
      "[120]\ttraining's auc: 0.787675\n",
      "[125]\ttraining's auc: 0.78858\n",
      "[130]\ttraining's auc: 0.789514\n",
      "[135]\ttraining's auc: 0.790689\n",
      "[140]\ttraining's auc: 0.791259\n",
      "[145]\ttraining's auc: 0.792152\n",
      "[150]\ttraining's auc: 0.793046\n",
      "[155]\ttraining's auc: 0.793678\n",
      "[160]\ttraining's auc: 0.794607\n",
      "[165]\ttraining's auc: 0.795404\n",
      "[170]\ttraining's auc: 0.796085\n",
      "[175]\ttraining's auc: 0.79718\n",
      "[180]\ttraining's auc: 0.797937\n",
      "[185]\ttraining's auc: 0.798671\n",
      "[190]\ttraining's auc: 0.799438\n",
      "[195]\ttraining's auc: 0.800197\n",
      "[200]\ttraining's auc: 0.801088\n",
      "[205]\ttraining's auc: 0.801873\n",
      "[210]\ttraining's auc: 0.802582\n",
      "[215]\ttraining's auc: 0.803205\n",
      "[220]\ttraining's auc: 0.803792\n",
      "[225]\ttraining's auc: 0.80422\n",
      "[230]\ttraining's auc: 0.804687\n",
      "[235]\ttraining's auc: 0.805218\n",
      "[240]\ttraining's auc: 0.805798\n",
      "[245]\ttraining's auc: 0.80643\n",
      "[250]\ttraining's auc: 0.807065\n",
      "[255]\ttraining's auc: 0.807625\n",
      "[260]\ttraining's auc: 0.808225\n",
      "[265]\ttraining's auc: 0.808709\n",
      "[270]\ttraining's auc: 0.809352\n",
      "[275]\ttraining's auc: 0.809805\n",
      "[280]\ttraining's auc: 0.810183\n",
      "[285]\ttraining's auc: 0.810659\n",
      "[290]\ttraining's auc: 0.811223\n",
      "[295]\ttraining's auc: 0.811662\n",
      "[300]\ttraining's auc: 0.812136\n",
      "Making predictions and saving them...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Those parameters are almost out of hat, so feel free to play with them. I can tell\n",
    "#you, that if you do it right, you will get better results for sure ;)\n",
    "print('Training LGBM model...')\n",
    "params = {}\n",
    "params['learning_rate'] = 0.1\n",
    "params['application'] = 'binary'\n",
    "params['max_depth'] = 15\n",
    "params['num_leaves'] = 2**8 -30\n",
    "params['verbosity'] = 0\n",
    "params['metric'] = 'auc'\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=300, valid_sets=watchlist, \\\n",
    "verbose_eval=5)\n",
    "\n",
    "print('Making predictions and saving them...')\n",
    "p_test = model.predict(X_test)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "subm.to_csv('submission5.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n",
      "[10]\ttraining's binary_logloss: 0.619229\n",
      "[20]\ttraining's binary_logloss: 0.594848\n",
      "[30]\ttraining's binary_logloss: 0.582896\n",
      "[40]\ttraining's binary_logloss: 0.574793\n",
      "[50]\ttraining's binary_logloss: 0.569345\n",
      "[60]\ttraining's binary_logloss: 0.565246\n",
      "[70]\ttraining's binary_logloss: 0.562101\n",
      "[80]\ttraining's binary_logloss: 0.559826\n",
      "[90]\ttraining's binary_logloss: 0.557644\n",
      "[100]\ttraining's binary_logloss: 0.555548\n",
      "[110]\ttraining's binary_logloss: 0.553501\n",
      "[120]\ttraining's binary_logloss: 0.55116\n",
      "[130]\ttraining's binary_logloss: 0.548832\n",
      "[140]\ttraining's binary_logloss: 0.546775\n",
      "[150]\ttraining's binary_logloss: 0.544641\n",
      "[160]\ttraining's binary_logloss: 0.543229\n",
      "[170]\ttraining's binary_logloss: 0.541466\n",
      "[180]\ttraining's binary_logloss: 0.539796\n",
      "[190]\ttraining's binary_logloss: 0.537879\n",
      "[200]\ttraining's binary_logloss: 0.536011\n",
      "Making predictions and saving them...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Those parameters are almost out of hat, so feel free to play with them. I can tell\n",
    "#you, that if you do it right, you will get better results for sure ;)\n",
    "print('Training LGBM model...')\n",
    "params = {}\n",
    "params['learning_rate'] = 0.1\n",
    "params['application'] = 'binary'\n",
    "params['max_depth'] = 20\n",
    "params['num_leaves'] = 2**8 -30\n",
    "params['verbosity'] = 0\n",
    "params['metric'] = 'binary_logloss'\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=200, valid_sets=watchlist, \\\n",
    "verbose_eval=10)\n",
    "\n",
    "print('Making predictions and saving them...')\n",
    "p_test = model.predict(X_test)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "subm.to_csv('submission6.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n",
      "[10]\ttraining's binary_logloss: 0.619229\n",
      "[20]\ttraining's binary_logloss: 0.594848\n",
      "[30]\ttraining's binary_logloss: 0.582896\n",
      "[40]\ttraining's binary_logloss: 0.574793\n",
      "[50]\ttraining's binary_logloss: 0.569345\n",
      "[60]\ttraining's binary_logloss: 0.565246\n",
      "[70]\ttraining's binary_logloss: 0.562101\n",
      "[80]\ttraining's binary_logloss: 0.559826\n",
      "[90]\ttraining's binary_logloss: 0.557644\n",
      "[100]\ttraining's binary_logloss: 0.555548\n",
      "[110]\ttraining's binary_logloss: 0.553501\n",
      "[120]\ttraining's binary_logloss: 0.55116\n",
      "[130]\ttraining's binary_logloss: 0.548832\n",
      "[140]\ttraining's binary_logloss: 0.546775\n",
      "[150]\ttraining's binary_logloss: 0.544641\n",
      "[160]\ttraining's binary_logloss: 0.543229\n",
      "[170]\ttraining's binary_logloss: 0.541466\n",
      "[180]\ttraining's binary_logloss: 0.539796\n",
      "[190]\ttraining's binary_logloss: 0.537879\n",
      "[200]\ttraining's binary_logloss: 0.536011\n",
      "[210]\ttraining's binary_logloss: 0.534548\n",
      "[220]\ttraining's binary_logloss: 0.533242\n",
      "[230]\ttraining's binary_logloss: 0.531787\n",
      "[240]\ttraining's binary_logloss: 0.530334\n",
      "[250]\ttraining's binary_logloss: 0.528894\n",
      "[260]\ttraining's binary_logloss: 0.527643\n",
      "[270]\ttraining's binary_logloss: 0.526435\n",
      "[280]\ttraining's binary_logloss: 0.524827\n",
      "[290]\ttraining's binary_logloss: 0.52362\n",
      "[300]\ttraining's binary_logloss: 0.522094\n",
      "[310]\ttraining's binary_logloss: 0.520727\n",
      "[320]\ttraining's binary_logloss: 0.519594\n",
      "[330]\ttraining's binary_logloss: 0.518445\n",
      "[340]\ttraining's binary_logloss: 0.517044\n",
      "[350]\ttraining's binary_logloss: 0.516246\n",
      "[360]\ttraining's binary_logloss: 0.51502\n",
      "[370]\ttraining's binary_logloss: 0.513844\n",
      "[380]\ttraining's binary_logloss: 0.512775\n",
      "[390]\ttraining's binary_logloss: 0.51182\n",
      "[400]\ttraining's binary_logloss: 0.51095\n",
      "[410]\ttraining's binary_logloss: 0.509835\n",
      "[420]\ttraining's binary_logloss: 0.508714\n",
      "[430]\ttraining's binary_logloss: 0.507822\n",
      "[440]\ttraining's binary_logloss: 0.506657\n",
      "[450]\ttraining's binary_logloss: 0.505473\n",
      "[460]\ttraining's binary_logloss: 0.504779\n",
      "[470]\ttraining's binary_logloss: 0.504066\n",
      "[480]\ttraining's binary_logloss: 0.503064\n",
      "[490]\ttraining's binary_logloss: 0.501733\n",
      "[500]\ttraining's binary_logloss: 0.500859\n",
      "[510]\ttraining's binary_logloss: 0.500176\n",
      "[520]\ttraining's binary_logloss: 0.49936\n",
      "[530]\ttraining's binary_logloss: 0.498585\n",
      "[540]\ttraining's binary_logloss: 0.498041\n",
      "[550]\ttraining's binary_logloss: 0.497128\n",
      "[560]\ttraining's binary_logloss: 0.496253\n",
      "[570]\ttraining's binary_logloss: 0.495447\n",
      "[580]\ttraining's binary_logloss: 0.494545\n",
      "[590]\ttraining's binary_logloss: 0.493476\n",
      "[600]\ttraining's binary_logloss: 0.492603\n",
      "[610]\ttraining's binary_logloss: 0.491734\n",
      "[620]\ttraining's binary_logloss: 0.49117\n",
      "[630]\ttraining's binary_logloss: 0.490611\n",
      "[640]\ttraining's binary_logloss: 0.489875\n",
      "[650]\ttraining's binary_logloss: 0.489174\n",
      "[660]\ttraining's binary_logloss: 0.488464\n",
      "[670]\ttraining's binary_logloss: 0.487945\n",
      "[680]\ttraining's binary_logloss: 0.487274\n",
      "[690]\ttraining's binary_logloss: 0.486396\n",
      "[700]\ttraining's binary_logloss: 0.485705\n",
      "[710]\ttraining's binary_logloss: 0.485188\n",
      "[720]\ttraining's binary_logloss: 0.484655\n",
      "[730]\ttraining's binary_logloss: 0.483834\n",
      "[740]\ttraining's binary_logloss: 0.483221\n",
      "[750]\ttraining's binary_logloss: 0.482482\n",
      "[760]\ttraining's binary_logloss: 0.481933\n",
      "[770]\ttraining's binary_logloss: 0.481419\n",
      "[780]\ttraining's binary_logloss: 0.480621\n",
      "[790]\ttraining's binary_logloss: 0.480094\n",
      "[800]\ttraining's binary_logloss: 0.479611\n",
      "[810]\ttraining's binary_logloss: 0.47899\n",
      "[820]\ttraining's binary_logloss: 0.478286\n",
      "[830]\ttraining's binary_logloss: 0.477528\n",
      "[840]\ttraining's binary_logloss: 0.476923\n",
      "[850]\ttraining's binary_logloss: 0.47635\n",
      "[860]\ttraining's binary_logloss: 0.475829\n",
      "[870]\ttraining's binary_logloss: 0.475176\n",
      "[880]\ttraining's binary_logloss: 0.474522\n",
      "[890]\ttraining's binary_logloss: 0.473905\n",
      "[900]\ttraining's binary_logloss: 0.473301\n",
      "[910]\ttraining's binary_logloss: 0.472717\n",
      "[920]\ttraining's binary_logloss: 0.472114\n",
      "[930]\ttraining's binary_logloss: 0.471477\n",
      "[940]\ttraining's binary_logloss: 0.470863\n",
      "[950]\ttraining's binary_logloss: 0.470259\n",
      "[960]\ttraining's binary_logloss: 0.469675\n",
      "[970]\ttraining's binary_logloss: 0.469154\n",
      "[980]\ttraining's binary_logloss: 0.468583\n",
      "[990]\ttraining's binary_logloss: 0.467945\n",
      "[1000]\ttraining's binary_logloss: 0.467418\n",
      "Making predictions and saving them...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Those parameters are almost out of hat, so feel free to play with them. I can tell\n",
    "#you, that if you do it right, you will get better results for sure ;)\n",
    "print('Training LGBM model...')\n",
    "params = {}\n",
    "params['learning_rate'] = 0.1\n",
    "params['application'] = 'binary'\n",
    "params['max_depth'] = 20\n",
    "params['num_leaves'] = 2**8 -30\n",
    "params['verbosity'] = 0\n",
    "params['metric'] = 'binary_logloss'\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=1000, valid_sets=watchlist, \\\n",
    "verbose_eval=10)\n",
    "\n",
    "print('Making predictions and saving them...')\n",
    "p_test = model.predict(X_test)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "subm.to_csv('submission7.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n",
      "[10]\ttraining's binary_logloss: 0.662875\n",
      "[20]\ttraining's binary_logloss: 0.643093\n",
      "[30]\ttraining's binary_logloss: 0.629345\n",
      "[40]\ttraining's binary_logloss: 0.61939\n",
      "[50]\ttraining's binary_logloss: 0.61157\n",
      "[60]\ttraining's binary_logloss: 0.605788\n",
      "[70]\ttraining's binary_logloss: 0.60142\n",
      "[80]\ttraining's binary_logloss: 0.597818\n",
      "[90]\ttraining's binary_logloss: 0.594884\n",
      "[100]\ttraining's binary_logloss: 0.592423\n",
      "[110]\ttraining's binary_logloss: 0.590027\n",
      "[120]\ttraining's binary_logloss: 0.587727\n",
      "[130]\ttraining's binary_logloss: 0.585883\n",
      "[140]\ttraining's binary_logloss: 0.584095\n",
      "[150]\ttraining's binary_logloss: 0.582682\n",
      "[160]\ttraining's binary_logloss: 0.581275\n",
      "[170]\ttraining's binary_logloss: 0.580142\n",
      "[180]\ttraining's binary_logloss: 0.579405\n",
      "[190]\ttraining's binary_logloss: 0.578491\n",
      "[200]\ttraining's binary_logloss: 0.577531\n",
      "[210]\ttraining's binary_logloss: 0.576233\n",
      "[220]\ttraining's binary_logloss: 0.575507\n",
      "[230]\ttraining's binary_logloss: 0.574873\n",
      "[240]\ttraining's binary_logloss: 0.574117\n",
      "[250]\ttraining's binary_logloss: 0.573387\n",
      "[260]\ttraining's binary_logloss: 0.57264\n",
      "[270]\ttraining's binary_logloss: 0.571966\n",
      "[280]\ttraining's binary_logloss: 0.571269\n",
      "[290]\ttraining's binary_logloss: 0.570641\n",
      "[300]\ttraining's binary_logloss: 0.570118\n",
      "[310]\ttraining's binary_logloss: 0.569518\n",
      "[320]\ttraining's binary_logloss: 0.568954\n",
      "[330]\ttraining's binary_logloss: 0.568302\n",
      "[340]\ttraining's binary_logloss: 0.567671\n",
      "[350]\ttraining's binary_logloss: 0.567061\n",
      "[360]\ttraining's binary_logloss: 0.566369\n",
      "[370]\ttraining's binary_logloss: 0.565803\n",
      "[380]\ttraining's binary_logloss: 0.565272\n",
      "[390]\ttraining's binary_logloss: 0.564681\n",
      "[400]\ttraining's binary_logloss: 0.564058\n",
      "[410]\ttraining's binary_logloss: 0.563407\n",
      "[420]\ttraining's binary_logloss: 0.562632\n",
      "[430]\ttraining's binary_logloss: 0.562201\n",
      "[440]\ttraining's binary_logloss: 0.561708\n",
      "[450]\ttraining's binary_logloss: 0.561097\n",
      "[460]\ttraining's binary_logloss: 0.560442\n",
      "[470]\ttraining's binary_logloss: 0.559853\n",
      "[480]\ttraining's binary_logloss: 0.559256\n",
      "[490]\ttraining's binary_logloss: 0.558759\n",
      "[500]\ttraining's binary_logloss: 0.558199\n",
      "Making predictions and saving them...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Those parameters are almost out of hat, so feel free to play with them. I can tell\n",
    "#you, that if you do it right, you will get better results for sure ;)\n",
    "print('Training LGBM model...')\n",
    "params = {}\n",
    "params['learning_rate'] = 0.03\n",
    "params['application'] = 'binary'\n",
    "params['max_depth'] = 20\n",
    "params['num_leaves'] = 2**8 -30\n",
    "params['verbosity'] = 0\n",
    "params['metric'] = 'binary_logloss'\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=500, valid_sets=watchlist, \\\n",
    "verbose_eval=10)\n",
    "\n",
    "print('Making predictions and saving them...')\n",
    "p_test = model.predict(X_test)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "subm.to_csv('submission8.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n",
      "[10]\ttraining's binary_logloss: 0.662875\n",
      "[20]\ttraining's binary_logloss: 0.643093\n",
      "[30]\ttraining's binary_logloss: 0.629345\n",
      "[40]\ttraining's binary_logloss: 0.61939\n",
      "[50]\ttraining's binary_logloss: 0.61157\n",
      "[60]\ttraining's binary_logloss: 0.605788\n",
      "[70]\ttraining's binary_logloss: 0.60142\n",
      "[80]\ttraining's binary_logloss: 0.597818\n",
      "[90]\ttraining's binary_logloss: 0.594884\n",
      "[100]\ttraining's binary_logloss: 0.592423\n",
      "[110]\ttraining's binary_logloss: 0.590027\n",
      "[120]\ttraining's binary_logloss: 0.587727\n",
      "[130]\ttraining's binary_logloss: 0.585883\n",
      "[140]\ttraining's binary_logloss: 0.584095\n",
      "[150]\ttraining's binary_logloss: 0.582682\n",
      "[160]\ttraining's binary_logloss: 0.581275\n",
      "[170]\ttraining's binary_logloss: 0.580142\n",
      "[180]\ttraining's binary_logloss: 0.579405\n",
      "[190]\ttraining's binary_logloss: 0.578491\n",
      "[200]\ttraining's binary_logloss: 0.577531\n",
      "[210]\ttraining's binary_logloss: 0.576233\n",
      "[220]\ttraining's binary_logloss: 0.575507\n",
      "[230]\ttraining's binary_logloss: 0.574873\n",
      "[240]\ttraining's binary_logloss: 0.574117\n",
      "[250]\ttraining's binary_logloss: 0.573387\n",
      "[260]\ttraining's binary_logloss: 0.57264\n",
      "[270]\ttraining's binary_logloss: 0.571966\n",
      "[280]\ttraining's binary_logloss: 0.571269\n",
      "[290]\ttraining's binary_logloss: 0.570641\n",
      "[300]\ttraining's binary_logloss: 0.570118\n",
      "[310]\ttraining's binary_logloss: 0.569518\n",
      "[320]\ttraining's binary_logloss: 0.568954\n",
      "[330]\ttraining's binary_logloss: 0.568302\n",
      "[340]\ttraining's binary_logloss: 0.567671\n",
      "[350]\ttraining's binary_logloss: 0.567061\n",
      "[360]\ttraining's binary_logloss: 0.566369\n",
      "[370]\ttraining's binary_logloss: 0.565803\n",
      "[380]\ttraining's binary_logloss: 0.565272\n",
      "[390]\ttraining's binary_logloss: 0.564681\n",
      "[400]\ttraining's binary_logloss: 0.564058\n",
      "[410]\ttraining's binary_logloss: 0.563407\n",
      "[420]\ttraining's binary_logloss: 0.562632\n",
      "[430]\ttraining's binary_logloss: 0.562201\n",
      "[440]\ttraining's binary_logloss: 0.561708\n",
      "[450]\ttraining's binary_logloss: 0.561097\n",
      "[460]\ttraining's binary_logloss: 0.560442\n",
      "[470]\ttraining's binary_logloss: 0.559853\n",
      "[480]\ttraining's binary_logloss: 0.559256\n",
      "[490]\ttraining's binary_logloss: 0.558759\n",
      "[500]\ttraining's binary_logloss: 0.558199\n",
      "[510]\ttraining's binary_logloss: 0.557574\n",
      "[520]\ttraining's binary_logloss: 0.557066\n",
      "[530]\ttraining's binary_logloss: 0.556581\n",
      "[540]\ttraining's binary_logloss: 0.55606\n",
      "[550]\ttraining's binary_logloss: 0.555534\n",
      "[560]\ttraining's binary_logloss: 0.554939\n",
      "[570]\ttraining's binary_logloss: 0.554333\n",
      "[580]\ttraining's binary_logloss: 0.553768\n",
      "[590]\ttraining's binary_logloss: 0.553214\n",
      "[600]\ttraining's binary_logloss: 0.552696\n",
      "[610]\ttraining's binary_logloss: 0.551994\n",
      "[620]\ttraining's binary_logloss: 0.55143\n",
      "[630]\ttraining's binary_logloss: 0.550974\n",
      "[640]\ttraining's binary_logloss: 0.550503\n",
      "[650]\ttraining's binary_logloss: 0.550099\n",
      "[660]\ttraining's binary_logloss: 0.549539\n",
      "[670]\ttraining's binary_logloss: 0.549018\n",
      "[680]\ttraining's binary_logloss: 0.548666\n",
      "[690]\ttraining's binary_logloss: 0.548365\n",
      "[700]\ttraining's binary_logloss: 0.548012\n",
      "[710]\ttraining's binary_logloss: 0.547649\n",
      "[720]\ttraining's binary_logloss: 0.547324\n",
      "[730]\ttraining's binary_logloss: 0.546979\n",
      "[740]\ttraining's binary_logloss: 0.546607\n",
      "[750]\ttraining's binary_logloss: 0.546257\n",
      "[760]\ttraining's binary_logloss: 0.545768\n",
      "[770]\ttraining's binary_logloss: 0.54523\n",
      "[780]\ttraining's binary_logloss: 0.544539\n",
      "[790]\ttraining's binary_logloss: 0.543964\n",
      "[800]\ttraining's binary_logloss: 0.543536\n",
      "[810]\ttraining's binary_logloss: 0.543142\n",
      "[820]\ttraining's binary_logloss: 0.542643\n",
      "[830]\ttraining's binary_logloss: 0.542144\n",
      "[840]\ttraining's binary_logloss: 0.541769\n",
      "[850]\ttraining's binary_logloss: 0.541323\n",
      "[860]\ttraining's binary_logloss: 0.540802\n",
      "[870]\ttraining's binary_logloss: 0.540447\n",
      "[880]\ttraining's binary_logloss: 0.540038\n",
      "[890]\ttraining's binary_logloss: 0.539506\n",
      "[900]\ttraining's binary_logloss: 0.538996\n",
      "[910]\ttraining's binary_logloss: 0.538606\n",
      "[920]\ttraining's binary_logloss: 0.538127\n",
      "[930]\ttraining's binary_logloss: 0.537706\n",
      "[940]\ttraining's binary_logloss: 0.537323\n",
      "[950]\ttraining's binary_logloss: 0.537003\n",
      "[960]\ttraining's binary_logloss: 0.536643\n",
      "[970]\ttraining's binary_logloss: 0.536249\n",
      "[980]\ttraining's binary_logloss: 0.535898\n",
      "[990]\ttraining's binary_logloss: 0.535514\n",
      "[1000]\ttraining's binary_logloss: 0.535148\n",
      "Making predictions and saving them...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Those parameters are almost out of hat, so feel free to play with them. I can tell\n",
    "#you, that if you do it right, you will get better results for sure ;)\n",
    "print('Training LGBM model...')\n",
    "params = {}\n",
    "params['learning_rate'] = 0.03\n",
    "params['application'] = 'binary'\n",
    "params['max_depth'] = 15\n",
    "params['num_leaves'] = 2**8 -30\n",
    "params['verbosity'] = 0\n",
    "params['metric'] = 'binary_logloss'\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=1000, valid_sets=watchlist, \\\n",
    "verbose_eval=10)\n",
    "\n",
    "print('Making predictions and saving them...')\n",
    "p_test = model.predict(X_test)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "subm.to_csv('submission8.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n",
      "[10]\ttraining's binary_logloss: 0.660093\n",
      "[20]\ttraining's binary_logloss: 0.63876\n",
      "[30]\ttraining's binary_logloss: 0.623914\n",
      "[40]\ttraining's binary_logloss: 0.612749\n",
      "[50]\ttraining's binary_logloss: 0.604249\n",
      "[60]\ttraining's binary_logloss: 0.597918\n",
      "[70]\ttraining's binary_logloss: 0.592799\n",
      "[80]\ttraining's binary_logloss: 0.588661\n",
      "[90]\ttraining's binary_logloss: 0.584983\n",
      "[100]\ttraining's binary_logloss: 0.582332\n",
      "[110]\ttraining's binary_logloss: 0.579623\n",
      "[120]\ttraining's binary_logloss: 0.577152\n",
      "[130]\ttraining's binary_logloss: 0.574659\n",
      "[140]\ttraining's binary_logloss: 0.572839\n",
      "[150]\ttraining's binary_logloss: 0.571141\n",
      "[160]\ttraining's binary_logloss: 0.569543\n",
      "[170]\ttraining's binary_logloss: 0.568179\n",
      "[180]\ttraining's binary_logloss: 0.56674\n",
      "[190]\ttraining's binary_logloss: 0.565608\n",
      "[200]\ttraining's binary_logloss: 0.564333\n",
      "[210]\ttraining's binary_logloss: 0.563276\n",
      "[220]\ttraining's binary_logloss: 0.562329\n",
      "[230]\ttraining's binary_logloss: 0.561634\n",
      "[240]\ttraining's binary_logloss: 0.560959\n",
      "[250]\ttraining's binary_logloss: 0.560139\n",
      "[260]\ttraining's binary_logloss: 0.559023\n",
      "[270]\ttraining's binary_logloss: 0.558116\n",
      "[280]\ttraining's binary_logloss: 0.557477\n",
      "[290]\ttraining's binary_logloss: 0.556807\n",
      "[300]\ttraining's binary_logloss: 0.556119\n",
      "[310]\ttraining's binary_logloss: 0.55547\n",
      "[320]\ttraining's binary_logloss: 0.554805\n",
      "[330]\ttraining's binary_logloss: 0.554127\n",
      "[340]\ttraining's binary_logloss: 0.553477\n",
      "[350]\ttraining's binary_logloss: 0.552825\n",
      "[360]\ttraining's binary_logloss: 0.552094\n",
      "[370]\ttraining's binary_logloss: 0.551623\n",
      "[380]\ttraining's binary_logloss: 0.55113\n",
      "[390]\ttraining's binary_logloss: 0.550555\n",
      "[400]\ttraining's binary_logloss: 0.550045\n",
      "[410]\ttraining's binary_logloss: 0.549414\n",
      "[420]\ttraining's binary_logloss: 0.548789\n",
      "[430]\ttraining's binary_logloss: 0.547924\n",
      "[440]\ttraining's binary_logloss: 0.547342\n",
      "[450]\ttraining's binary_logloss: 0.546634\n",
      "[460]\ttraining's binary_logloss: 0.545786\n",
      "[470]\ttraining's binary_logloss: 0.545058\n",
      "[480]\ttraining's binary_logloss: 0.544323\n",
      "[490]\ttraining's binary_logloss: 0.543662\n",
      "[500]\ttraining's binary_logloss: 0.543191\n",
      "[510]\ttraining's binary_logloss: 0.542702\n",
      "[520]\ttraining's binary_logloss: 0.542208\n",
      "[530]\ttraining's binary_logloss: 0.541632\n",
      "[540]\ttraining's binary_logloss: 0.541012\n",
      "[550]\ttraining's binary_logloss: 0.540482\n",
      "[560]\ttraining's binary_logloss: 0.539955\n",
      "[570]\ttraining's binary_logloss: 0.539344\n",
      "[580]\ttraining's binary_logloss: 0.538743\n",
      "[590]\ttraining's binary_logloss: 0.53825\n",
      "[600]\ttraining's binary_logloss: 0.537655\n",
      "[610]\ttraining's binary_logloss: 0.537094\n",
      "[620]\ttraining's binary_logloss: 0.536658\n",
      "[630]\ttraining's binary_logloss: 0.536111\n",
      "[640]\ttraining's binary_logloss: 0.535546\n",
      "[650]\ttraining's binary_logloss: 0.53514\n",
      "[660]\ttraining's binary_logloss: 0.534755\n",
      "[670]\ttraining's binary_logloss: 0.534415\n",
      "[680]\ttraining's binary_logloss: 0.533975\n",
      "[690]\ttraining's binary_logloss: 0.533524\n",
      "[700]\ttraining's binary_logloss: 0.532908\n",
      "[710]\ttraining's binary_logloss: 0.532377\n",
      "[720]\ttraining's binary_logloss: 0.531889\n",
      "[730]\ttraining's binary_logloss: 0.531454\n",
      "[740]\ttraining's binary_logloss: 0.530856\n",
      "[750]\ttraining's binary_logloss: 0.530456\n",
      "[760]\ttraining's binary_logloss: 0.530003\n",
      "[770]\ttraining's binary_logloss: 0.529563\n",
      "[780]\ttraining's binary_logloss: 0.529126\n",
      "[790]\ttraining's binary_logloss: 0.528698\n",
      "[800]\ttraining's binary_logloss: 0.52812\n",
      "[810]\ttraining's binary_logloss: 0.527652\n",
      "[820]\ttraining's binary_logloss: 0.527159\n",
      "[830]\ttraining's binary_logloss: 0.526721\n",
      "[840]\ttraining's binary_logloss: 0.526322\n",
      "[850]\ttraining's binary_logloss: 0.525891\n",
      "[860]\ttraining's binary_logloss: 0.52552\n",
      "[870]\ttraining's binary_logloss: 0.525072\n",
      "[880]\ttraining's binary_logloss: 0.524647\n",
      "[890]\ttraining's binary_logloss: 0.524298\n",
      "[900]\ttraining's binary_logloss: 0.523819\n",
      "[910]\ttraining's binary_logloss: 0.523429\n",
      "[920]\ttraining's binary_logloss: 0.522972\n",
      "[930]\ttraining's binary_logloss: 0.522671\n",
      "[940]\ttraining's binary_logloss: 0.522324\n",
      "[950]\ttraining's binary_logloss: 0.521985\n",
      "[960]\ttraining's binary_logloss: 0.521637\n",
      "[970]\ttraining's binary_logloss: 0.521342\n",
      "[980]\ttraining's binary_logloss: 0.521045\n",
      "[990]\ttraining's binary_logloss: 0.520696\n",
      "[1000]\ttraining's binary_logloss: 0.520303\n",
      "[1010]\ttraining's binary_logloss: 0.519876\n",
      "[1020]\ttraining's binary_logloss: 0.519417\n",
      "[1030]\ttraining's binary_logloss: 0.519026\n",
      "[1040]\ttraining's binary_logloss: 0.518607\n",
      "[1050]\ttraining's binary_logloss: 0.518294\n",
      "[1060]\ttraining's binary_logloss: 0.517957\n",
      "[1070]\ttraining's binary_logloss: 0.51753\n",
      "[1080]\ttraining's binary_logloss: 0.517172\n",
      "[1090]\ttraining's binary_logloss: 0.516914\n",
      "[1100]\ttraining's binary_logloss: 0.516538\n",
      "[1110]\ttraining's binary_logloss: 0.516165\n",
      "[1120]\ttraining's binary_logloss: 0.515794\n",
      "[1130]\ttraining's binary_logloss: 0.515342\n",
      "[1140]\ttraining's binary_logloss: 0.514949\n",
      "[1150]\ttraining's binary_logloss: 0.514523\n",
      "[1160]\ttraining's binary_logloss: 0.514129\n",
      "[1170]\ttraining's binary_logloss: 0.513603\n",
      "[1180]\ttraining's binary_logloss: 0.513335\n",
      "[1190]\ttraining's binary_logloss: 0.513087\n",
      "[1200]\ttraining's binary_logloss: 0.512808\n",
      "[1210]\ttraining's binary_logloss: 0.512527\n",
      "[1220]\ttraining's binary_logloss: 0.512307\n",
      "[1230]\ttraining's binary_logloss: 0.512079\n",
      "[1240]\ttraining's binary_logloss: 0.511786\n",
      "[1250]\ttraining's binary_logloss: 0.511426\n",
      "[1260]\ttraining's binary_logloss: 0.511053\n",
      "[1270]\ttraining's binary_logloss: 0.510808\n",
      "[1280]\ttraining's binary_logloss: 0.51057\n",
      "[1290]\ttraining's binary_logloss: 0.510211\n",
      "[1300]\ttraining's binary_logloss: 0.509939\n",
      "[1310]\ttraining's binary_logloss: 0.509696\n",
      "[1320]\ttraining's binary_logloss: 0.509357\n",
      "[1330]\ttraining's binary_logloss: 0.509026\n",
      "[1340]\ttraining's binary_logloss: 0.508707\n",
      "[1350]\ttraining's binary_logloss: 0.508336\n",
      "[1360]\ttraining's binary_logloss: 0.507909\n",
      "[1370]\ttraining's binary_logloss: 0.507523\n",
      "[1380]\ttraining's binary_logloss: 0.507089\n",
      "[1390]\ttraining's binary_logloss: 0.506738\n",
      "[1400]\ttraining's binary_logloss: 0.506459\n",
      "[1410]\ttraining's binary_logloss: 0.506115\n",
      "[1420]\ttraining's binary_logloss: 0.505866\n",
      "[1430]\ttraining's binary_logloss: 0.505675\n",
      "[1440]\ttraining's binary_logloss: 0.505456\n",
      "[1450]\ttraining's binary_logloss: 0.505063\n",
      "[1460]\ttraining's binary_logloss: 0.504755\n",
      "[1470]\ttraining's binary_logloss: 0.504472\n",
      "[1480]\ttraining's binary_logloss: 0.504064\n",
      "[1490]\ttraining's binary_logloss: 0.503723\n",
      "[1500]\ttraining's binary_logloss: 0.503336\n",
      "Making predictions and saving them...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Those parameters are almost out of hat, so feel free to play with them. I can tell\n",
    "#you, that if you do it right, you will get better results for sure ;)\n",
    "print('Training LGBM model...')\n",
    "params = {}\n",
    "params['learning_rate'] = 0.03\n",
    "params['application'] = 'binary'\n",
    "params['max_depth'] = 20\n",
    "params['num_leaves'] = 2**8 -30\n",
    "params['verbosity'] = 0\n",
    "params['metric'] = 'binary_logloss'\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=1500, valid_sets=watchlist, \\\n",
    "verbose_eval=10)\n",
    "\n",
    "print('Making predictions and saving them...')\n",
    "p_test = model.predict(X_test)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "subm.to_csv('submission8.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n",
      "[10]\ttraining's binary_logloss: 0.660093\n",
      "[20]\ttraining's binary_logloss: 0.63876\n",
      "[30]\ttraining's binary_logloss: 0.623914\n",
      "[40]\ttraining's binary_logloss: 0.612749\n",
      "[50]\ttraining's binary_logloss: 0.604249\n",
      "[60]\ttraining's binary_logloss: 0.597918\n",
      "[70]\ttraining's binary_logloss: 0.592799\n",
      "[80]\ttraining's binary_logloss: 0.588661\n",
      "[90]\ttraining's binary_logloss: 0.584983\n",
      "[100]\ttraining's binary_logloss: 0.582332\n",
      "[110]\ttraining's binary_logloss: 0.579623\n",
      "[120]\ttraining's binary_logloss: 0.577152\n",
      "[130]\ttraining's binary_logloss: 0.574659\n",
      "[140]\ttraining's binary_logloss: 0.572839\n",
      "[150]\ttraining's binary_logloss: 0.571141\n",
      "[160]\ttraining's binary_logloss: 0.569543\n",
      "[170]\ttraining's binary_logloss: 0.568179\n",
      "[180]\ttraining's binary_logloss: 0.56674\n",
      "[190]\ttraining's binary_logloss: 0.565608\n",
      "[200]\ttraining's binary_logloss: 0.564333\n",
      "[210]\ttraining's binary_logloss: 0.563276\n",
      "[220]\ttraining's binary_logloss: 0.562329\n",
      "[230]\ttraining's binary_logloss: 0.561634\n",
      "[240]\ttraining's binary_logloss: 0.560959\n",
      "[250]\ttraining's binary_logloss: 0.560139\n",
      "[260]\ttraining's binary_logloss: 0.559023\n",
      "[270]\ttraining's binary_logloss: 0.558116\n",
      "[280]\ttraining's binary_logloss: 0.557477\n",
      "[290]\ttraining's binary_logloss: 0.556807\n",
      "[300]\ttraining's binary_logloss: 0.556119\n",
      "[310]\ttraining's binary_logloss: 0.55547\n",
      "[320]\ttraining's binary_logloss: 0.554805\n",
      "[330]\ttraining's binary_logloss: 0.554127\n",
      "[340]\ttraining's binary_logloss: 0.553477\n",
      "[350]\ttraining's binary_logloss: 0.552825\n",
      "[360]\ttraining's binary_logloss: 0.552094\n",
      "[370]\ttraining's binary_logloss: 0.551623\n",
      "[380]\ttraining's binary_logloss: 0.55113\n",
      "[390]\ttraining's binary_logloss: 0.550555\n",
      "[400]\ttraining's binary_logloss: 0.550045\n",
      "[410]\ttraining's binary_logloss: 0.549414\n",
      "[420]\ttraining's binary_logloss: 0.548789\n",
      "[430]\ttraining's binary_logloss: 0.547924\n",
      "[440]\ttraining's binary_logloss: 0.547342\n",
      "[450]\ttraining's binary_logloss: 0.546634\n",
      "[460]\ttraining's binary_logloss: 0.545786\n",
      "[470]\ttraining's binary_logloss: 0.545058\n",
      "[480]\ttraining's binary_logloss: 0.544323\n",
      "[490]\ttraining's binary_logloss: 0.543662\n",
      "[500]\ttraining's binary_logloss: 0.543191\n",
      "[510]\ttraining's binary_logloss: 0.542702\n",
      "[520]\ttraining's binary_logloss: 0.542208\n",
      "[530]\ttraining's binary_logloss: 0.541632\n",
      "[540]\ttraining's binary_logloss: 0.541012\n",
      "[550]\ttraining's binary_logloss: 0.540482\n",
      "[560]\ttraining's binary_logloss: 0.539955\n",
      "[570]\ttraining's binary_logloss: 0.539344\n",
      "[580]\ttraining's binary_logloss: 0.538743\n",
      "[590]\ttraining's binary_logloss: 0.53825\n",
      "[600]\ttraining's binary_logloss: 0.537655\n",
      "[610]\ttraining's binary_logloss: 0.537094\n",
      "[620]\ttraining's binary_logloss: 0.536658\n",
      "[630]\ttraining's binary_logloss: 0.536111\n",
      "[640]\ttraining's binary_logloss: 0.535546\n",
      "[650]\ttraining's binary_logloss: 0.53514\n",
      "[660]\ttraining's binary_logloss: 0.534755\n",
      "[670]\ttraining's binary_logloss: 0.534415\n",
      "[680]\ttraining's binary_logloss: 0.533975\n",
      "[690]\ttraining's binary_logloss: 0.533524\n",
      "[700]\ttraining's binary_logloss: 0.532908\n",
      "[710]\ttraining's binary_logloss: 0.532377\n",
      "[720]\ttraining's binary_logloss: 0.531889\n",
      "[730]\ttraining's binary_logloss: 0.531454\n",
      "[740]\ttraining's binary_logloss: 0.530856\n",
      "[750]\ttraining's binary_logloss: 0.530456\n",
      "[760]\ttraining's binary_logloss: 0.530003\n",
      "[770]\ttraining's binary_logloss: 0.529563\n",
      "[780]\ttraining's binary_logloss: 0.529126\n",
      "[790]\ttraining's binary_logloss: 0.528698\n",
      "[800]\ttraining's binary_logloss: 0.52812\n",
      "[810]\ttraining's binary_logloss: 0.527652\n",
      "[820]\ttraining's binary_logloss: 0.527159\n",
      "[830]\ttraining's binary_logloss: 0.526721\n",
      "[840]\ttraining's binary_logloss: 0.526322\n",
      "[850]\ttraining's binary_logloss: 0.525891\n",
      "[860]\ttraining's binary_logloss: 0.52552\n",
      "[870]\ttraining's binary_logloss: 0.525072\n",
      "[880]\ttraining's binary_logloss: 0.524647\n",
      "[890]\ttraining's binary_logloss: 0.524298\n",
      "[900]\ttraining's binary_logloss: 0.523819\n",
      "[910]\ttraining's binary_logloss: 0.523429\n",
      "[920]\ttraining's binary_logloss: 0.522972\n",
      "[930]\ttraining's binary_logloss: 0.522671\n",
      "[940]\ttraining's binary_logloss: 0.522324\n",
      "[950]\ttraining's binary_logloss: 0.521985\n",
      "[960]\ttraining's binary_logloss: 0.521637\n",
      "[970]\ttraining's binary_logloss: 0.521342\n",
      "[980]\ttraining's binary_logloss: 0.521045\n",
      "[990]\ttraining's binary_logloss: 0.520696\n",
      "[1000]\ttraining's binary_logloss: 0.520303\n",
      "[1010]\ttraining's binary_logloss: 0.519876\n",
      "[1020]\ttraining's binary_logloss: 0.519417\n",
      "[1030]\ttraining's binary_logloss: 0.519026\n",
      "[1040]\ttraining's binary_logloss: 0.518607\n",
      "[1050]\ttraining's binary_logloss: 0.518294\n",
      "[1060]\ttraining's binary_logloss: 0.517957\n",
      "[1070]\ttraining's binary_logloss: 0.51753\n",
      "[1080]\ttraining's binary_logloss: 0.517172\n",
      "[1090]\ttraining's binary_logloss: 0.516914\n",
      "[1100]\ttraining's binary_logloss: 0.516538\n",
      "[1110]\ttraining's binary_logloss: 0.516165\n",
      "[1120]\ttraining's binary_logloss: 0.515794\n",
      "[1130]\ttraining's binary_logloss: 0.515342\n",
      "[1140]\ttraining's binary_logloss: 0.514949\n",
      "[1150]\ttraining's binary_logloss: 0.514523\n",
      "[1160]\ttraining's binary_logloss: 0.514129\n",
      "[1170]\ttraining's binary_logloss: 0.513603\n",
      "[1180]\ttraining's binary_logloss: 0.513335\n",
      "[1190]\ttraining's binary_logloss: 0.513087\n",
      "[1200]\ttraining's binary_logloss: 0.512808\n",
      "[1210]\ttraining's binary_logloss: 0.512527\n",
      "[1220]\ttraining's binary_logloss: 0.512307\n",
      "[1230]\ttraining's binary_logloss: 0.512079\n",
      "[1240]\ttraining's binary_logloss: 0.511786\n",
      "[1250]\ttraining's binary_logloss: 0.511426\n",
      "[1260]\ttraining's binary_logloss: 0.511053\n",
      "[1270]\ttraining's binary_logloss: 0.510808\n",
      "[1280]\ttraining's binary_logloss: 0.51057\n",
      "[1290]\ttraining's binary_logloss: 0.510211\n",
      "[1300]\ttraining's binary_logloss: 0.509939\n",
      "[1310]\ttraining's binary_logloss: 0.509696\n",
      "[1320]\ttraining's binary_logloss: 0.509357\n",
      "[1330]\ttraining's binary_logloss: 0.509026\n",
      "[1340]\ttraining's binary_logloss: 0.508707\n",
      "[1350]\ttraining's binary_logloss: 0.508336\n",
      "[1360]\ttraining's binary_logloss: 0.507909\n",
      "[1370]\ttraining's binary_logloss: 0.507523\n",
      "[1380]\ttraining's binary_logloss: 0.507089\n",
      "[1390]\ttraining's binary_logloss: 0.506738\n",
      "[1400]\ttraining's binary_logloss: 0.506459\n",
      "[1410]\ttraining's binary_logloss: 0.506115\n",
      "[1420]\ttraining's binary_logloss: 0.505866\n",
      "[1430]\ttraining's binary_logloss: 0.505675\n",
      "[1440]\ttraining's binary_logloss: 0.505456\n",
      "[1450]\ttraining's binary_logloss: 0.505063\n",
      "[1460]\ttraining's binary_logloss: 0.504755\n",
      "[1470]\ttraining's binary_logloss: 0.504472\n",
      "[1480]\ttraining's binary_logloss: 0.504064\n",
      "[1490]\ttraining's binary_logloss: 0.503723\n",
      "[1500]\ttraining's binary_logloss: 0.503336\n",
      "[1510]\ttraining's binary_logloss: 0.502945\n",
      "[1520]\ttraining's binary_logloss: 0.5026\n",
      "[1530]\ttraining's binary_logloss: 0.502263\n",
      "[1540]\ttraining's binary_logloss: 0.501881\n",
      "[1550]\ttraining's binary_logloss: 0.5016\n",
      "[1560]\ttraining's binary_logloss: 0.501293\n",
      "[1570]\ttraining's binary_logloss: 0.500963\n",
      "[1580]\ttraining's binary_logloss: 0.50064\n",
      "[1590]\ttraining's binary_logloss: 0.500314\n",
      "[1600]\ttraining's binary_logloss: 0.500058\n",
      "[1610]\ttraining's binary_logloss: 0.499867\n",
      "[1620]\ttraining's binary_logloss: 0.499679\n",
      "[1630]\ttraining's binary_logloss: 0.4995\n",
      "[1640]\ttraining's binary_logloss: 0.499319\n",
      "[1650]\ttraining's binary_logloss: 0.499139\n",
      "[1660]\ttraining's binary_logloss: 0.498756\n",
      "[1670]\ttraining's binary_logloss: 0.498433\n",
      "[1680]\ttraining's binary_logloss: 0.498157\n",
      "[1690]\ttraining's binary_logloss: 0.497784\n",
      "[1700]\ttraining's binary_logloss: 0.4975\n",
      "[1710]\ttraining's binary_logloss: 0.497165\n",
      "[1720]\ttraining's binary_logloss: 0.496847\n",
      "[1730]\ttraining's binary_logloss: 0.496633\n",
      "[1740]\ttraining's binary_logloss: 0.496369\n",
      "[1750]\ttraining's binary_logloss: 0.49619\n",
      "[1760]\ttraining's binary_logloss: 0.495886\n",
      "[1770]\ttraining's binary_logloss: 0.495639\n",
      "[1780]\ttraining's binary_logloss: 0.495332\n",
      "[1790]\ttraining's binary_logloss: 0.495038\n",
      "[1800]\ttraining's binary_logloss: 0.494789\n",
      "Making predictions and saving them...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Those parameters are almost out of hat, so feel free to play with them. I can tell\n",
    "#you, that if you do it right, you will get better results for sure ;)\n",
    "print('Training LGBM model...')\n",
    "params = {}\n",
    "params['learning_rate'] = 0.03\n",
    "params['application'] = 'binary'\n",
    "params['max_depth'] = 20\n",
    "params['num_leaves'] = 2**8 -30\n",
    "params['verbosity'] = 0\n",
    "params['metric'] = 'binary_logloss'\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=1800, valid_sets=watchlist, \\\n",
    "verbose_eval=10)\n",
    "\n",
    "print('Making predictions and saving them...')\n",
    "p_test = model.predict(X_test)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "subm.to_csv('submission9.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n",
      "[10]\tvalid_0's binary_logloss: 0.659504\n",
      "[20]\tvalid_0's binary_logloss: 0.657054\n",
      "[30]\tvalid_0's binary_logloss: 0.65582\n",
      "[40]\tvalid_0's binary_logloss: 0.654058\n",
      "[50]\tvalid_0's binary_logloss: 0.653354\n",
      "[60]\tvalid_0's binary_logloss: 0.652457\n",
      "[70]\tvalid_0's binary_logloss: 0.652134\n",
      "[80]\tvalid_0's binary_logloss: 0.651703\n",
      "[90]\tvalid_0's binary_logloss: 0.651506\n",
      "[100]\tvalid_0's binary_logloss: 0.651359\n",
      "[110]\tvalid_0's binary_logloss: 0.651298\n",
      "[120]\tvalid_0's binary_logloss: 0.651148\n",
      "[130]\tvalid_0's binary_logloss: 0.650944\n",
      "[140]\tvalid_0's binary_logloss: 0.650653\n",
      "[150]\tvalid_0's binary_logloss: 0.650657\n",
      "[160]\tvalid_0's binary_logloss: 0.65058\n",
      "[170]\tvalid_0's binary_logloss: 0.650647\n",
      "[180]\tvalid_0's binary_logloss: 0.650756\n",
      "[190]\tvalid_0's binary_logloss: 0.650747\n",
      "[200]\tvalid_0's binary_logloss: 0.650721\n"
     ]
    }
   ],
   "source": [
    "#Those parameters are almost out of hat, so feel free to play with them. I can tell\n",
    "#you, that if you do it right, you will get better results for sure ;)\n",
    "print('Training LGBM model...')\n",
    "params = {}\n",
    "params['learning_rate'] = 0.1\n",
    "params['application'] = 'binary'\n",
    "params['max_depth'] = 20\n",
    "params['num_leaves'] = 2**8 -30\n",
    "params['verbosity'] = 0\n",
    "params['metric'] = 'binary_logloss'\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=200, valid_sets=dev, \\\n",
    "verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_val = model.predict(X1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.46302668,  0.42956573,  0.77592151, ...,  0.76529591,\n",
       "        0.74784553,  0.74584525])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convet(one):\n",
    "    if one > 0.6 :\n",
    "        one = 1 \n",
    "    else:\n",
    "        one = 0 \n",
    "    return one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_co =list(map(convet,p_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_co = np.array(p_co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xxx = list(map(lambda x : x*x, y1 - p_co))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xxx = np.array(xxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64129510497346409"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - xxx.sum()/len(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=uint64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y1).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "p_test = model.predict(X_test)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "subm.to_csv('submissionix.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "roc_auc_score() missing 1 required positional argument: 'y_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-f2e35d96ca3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: roc_auc_score() missing 1 required positional argument: 'y_score'"
     ]
    }
   ],
   "source": [
    "roc_auc_score((p_val,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-68d3e5a014c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'y_score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "p_val = Dataframe(p_val,column = 'y_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.46302668,  0.42956573,  0.77592151, ...,  0.76529591,\n",
       "        0.74784553,  0.74584525])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67389369529715615"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y1, p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n",
      "[10]\tvalid_0's auc: 0.693619\n",
      "[20]\tvalid_0's auc: 0.709217\n",
      "[30]\tvalid_0's auc: 0.718852\n",
      "[40]\tvalid_0's auc: 0.725644\n",
      "[50]\tvalid_0's auc: 0.730436\n",
      "[60]\tvalid_0's auc: 0.732827\n",
      "[70]\tvalid_0's auc: 0.73532\n",
      "[80]\tvalid_0's auc: 0.737723\n",
      "[90]\tvalid_0's auc: 0.74062\n",
      "[100]\tvalid_0's auc: 0.74343\n",
      "[110]\tvalid_0's auc: 0.745752\n",
      "[120]\tvalid_0's auc: 0.74783\n",
      "[130]\tvalid_0's auc: 0.750083\n",
      "[140]\tvalid_0's auc: 0.752044\n",
      "[150]\tvalid_0's auc: 0.754218\n",
      "[160]\tvalid_0's auc: 0.756075\n",
      "[170]\tvalid_0's auc: 0.757757\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5f5e1efafb96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metric'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    197\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1437\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1438\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Those parameters are almost out of hat, so feel free to play with them. I can tell\n",
    "#you, that if you do it right, you will get better results for sure ;)\n",
    "print('Training LGBM model...')\n",
    "params = {}\n",
    "params['learning_rate'] = 0.1\n",
    "params['application'] = 'binary'\n",
    "params['max_depth'] = 15\n",
    "params['num_leaves'] = 2**8 -30\n",
    "params['verbosity'] = 0\n",
    "params['metric'] = 'auc'\n",
    "\n",
    "model = lgb.train(params, train_set=d_train, num_boost_round=200, valid_sets=dev, \\\n",
    "verbose_eval=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions and saving them...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Making predictions and saving them...')\n",
    "p_test = model.predict(X_test)\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "subm.to_csv('submission11.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
